---
title: "Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval"
date: "2024-08-06"
authors: "Iman Azimi, Mohan Qi, Li Wang, Amir M, Rahmani Youlin, and Li"
---

import Image from 'next/image';

<div className="bg-gray-800 text-white min-h-screen p-8">
  <h1 className="text-4xl font-bold mb-6">Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval</h1>

  <div className="mb-8">
    {{/* eslint-disable-next-line @next/next/no-img-element */}}
    <img
      src="/images/papers/Accuracy and Consistency of LLMs in the Registered Dietitian Exam_ The Impact of Prompt Engineering .png"
      alt="Paper image"
      className="w-full h-auto rounded-lg"
    />
    <p className="text-sm text-gray-400 mt-2">1.5 Pro, CoT-SC
ni 1.5 Pro, RAP
2
5 0.85
0.84
0.86
0.88
0.90
0.92
0.94
0.96
</p>
  </div>

  <p className="text-gray-300 mb-2"><strong>Date:</strong> 2024-08-06</p>
  <p className="text-gray-300 italic mb-6"><strong>Authors:</strong> Iman Azimi, Mohan Qi, Li Wang, Amir M, Rahmani Youlin, and Li</p>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">Abstract</h2>
    <p>Large language models (LLMs) are fundamentally transforming human-facing
applications in the health and well-being domains: boosting patient engagement,
accelerating clinical decision-making, and facilitating medical education.
Although state-of-the-art LLMs have shown superior performance in several
conversational applications, evaluations within nutrition and diet applications
are still insufficient. In this paper, we propose to employ the Registered
Dietitian (RD) exam to conduct a standard and comprehensive evaluation of
state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing
both accuracy and consistency in nutrition queries. Our evaluation includes
1050 RD exam questions encompassing several nutrition topics and proficiency
levels. In addition, for the first time, we examine the impact of Zero-Shot
(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),
and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the
responses. Our findings revealed that while these LLMs obtained acceptable
overall performance, their results varied considerably with different prompts
and question domains. GPT-4o with CoT-SC prompting outperformed the other
approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.
For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both
accuracy and consistency. RAP was particularly effective for GPT-4o to answer
Expert level questions. Consequently, choosing the appropriate LLM and
prompting technique, tailored to the proficiency level and specific domain, can
mitigate errors and potential risks in diet and nutrition chatbots.</p>
  </div>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">AI-Generated Summary</h2>
    <p>The paper explicitly discusses several prompting techniques including Zero-Shot, Chain of Thought, Chain of Thought with Self Consistency, and Retrieval Augmented Prompting, predominantly in the context of hard prefix prompts for Large Language Models.</p>
  </div>

  <p className="text-sm text-gray-400 mt-8 mb-8">
    The summary is AI-generated and may not perfectly reflect the paper's content.
  </p>

  <div className="flex justify-center">
    
      href="http://arxiv.org/pdf/2408.02964v1"
      className="bg-green-500 text-white py-3 px-6 rounded-lg hover:bg-green-600 transition duration-300 inline-block text-center"
      target="_blank"
      rel="noopener noreferrer"
    >
      Read Full Paper
    </a>
  </div>
</div>
