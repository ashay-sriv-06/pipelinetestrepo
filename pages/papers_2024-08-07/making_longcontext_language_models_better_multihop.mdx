---
title: "Making Long-Context Language Models Better Multi-Hop Reasoners"
date: "2024-08-06"
authors: "[, ', Y, a, n, y, a, n, g,  , L, i, ', ,,  , ', S, h, u, o,  , L, i, a, n, g, ', ,,  , ', M, i, c, h, a, e, l,  , R, .,  , L, y, u, ', ,,  , ', L, i, w, e, i,  , W, a, n, g, ', ]"
---

import Image from 'next/image';

<div className="bg-gray-800 text-white min-h-screen p-8">
  <h1 className="text-4xl font-bold mb-6">Making Long-Context Language Models Better Multi-Hop Reasoners</h1>

  <div className="mb-8">
    <Image
      src="/images/papers/Making Long-Context Language Models Better Multi-Hop Reasoners.png"
      alt="Paper image"
      width={800}
      height={600}
      layout="responsive"
      className="rounded-lg"
    />
    <p className="text-sm text-gray-400 mt-2">Figure 5: Screenshot of our human annotation tool.
Citation Precision
Citation Recall
</p>
  </div>

  <p className="text-gray-300 mb-2"><strong>Date:</strong> 2024-08-06</p>
  <p className="text-gray-300 italic mb-6"><strong>Authors:</strong> [, ', Y, a, n, y, a, n, g,  , L, i, ', ,,  , ', S, h, u, o,  , L, i, a, n, g, ', ,,  , ', M, i, c, h, a, e, l,  , R, .,  , L, y, u, ', ,,  , ', L, i, w, e, i,  , W, a, n, g, ', ]</p>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">Abstract</h2>
    <p>Recent advancements in long-context modeling have enhanced language models
(LMs) for complex tasks across multiple NLP applications. Despite this
progress, we find that these models struggle with multi-hop reasoning and
exhibit decreased performance in the presence of noisy contexts. In this paper,
we introduce Reasoning with Attributions, a novel approach that prompts LMs to
supply attributions for each assertion during their reasoning. We validate our
approach through experiments on three multi-hop datasets, employing both
proprietary and open-source models, and demonstrate its efficacy and
resilience. Furthermore, we explore methods to augment reasoning capabilities
via fine-tuning and offer an attribution-annotated dataset and a specialized
training strategy. Our fine-tuned model achieves competitive performance on
multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as
ChatGPT and Claude-instant.</p>
  </div>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">AI-Generated Summary</h2>
    <p>The paper introduces a novel approach prompting language models and also includes methods to augment reasoning capabilities via post-training fine-tuning.</p>
  </div>


    href="http://arxiv.org/pdf/2408.03246v1"
    className="bg-blue-500 text-white py-3 px-6 rounded-lg hover:bg-blue-600 transition duration-300 inline-block"
    target="_blank"
    rel="noopener noreferrer"
  >
    Read the full paper
  </a>

  <p className="text-sm text-gray-400 mt-8">
    The summary is AI-generated and may not perfectly reflect the paper's content.
  </p>
</div>
