---
title: "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models"
date: "2024-08-05"
authors: "[, ', M, u, x, i,  , D, i, a, o, ', ,,  , ', R, u, m, e, i,  , L, i, ', ,,  , ', S, h, i, y, a, n, g,  , L, i, u, ', ,,  , ', G, u, o, g, a, n, g,  , L, i, a, o, ', ,,  , ', J, i, n, g, a, n, g,  , W, a, n, g, ', ,,  , ', X, u, n, l, i, a, n, g,  , C, a, i, ', ,,  , ', W, e, i, r, a, n,  , X, u, ', ]"
---

import Image from 'next/image';

<div className="bg-gray-800 text-white min-h-screen p-8">
  <h1 className="text-4xl font-bold mb-6">SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models</h1>

  <div className="mb-8">
    <Image
      src="/images/papers/SEAS_ Self-Evolving Adversarial Safety Optimization for Large Language Models.png"
      alt="Paper image"
      width={800}
      height={600}
      layout="responsive"
      className="rounded-lg"
    />
    <p className="text-sm text-gray-400 mt-2">nan</p>
  </div>

  <p className="text-gray-300 mb-2"><strong>Date:</strong> 2024-08-05</p>
  <p className="text-gray-300 italic mb-6"><strong>Authors:</strong> [, ', M, u, x, i,  , D, i, a, o, ', ,,  , ', R, u, m, e, i,  , L, i, ', ,,  , ', S, h, i, y, a, n, g,  , L, i, u, ', ,,  , ', G, u, o, g, a, n, g,  , L, i, a, o, ', ,,  , ', J, i, n, g, a, n, g,  , W, a, n, g, ', ,,  , ', X, u, n, l, i, a, n, g,  , C, a, i, ', ,,  , ', W, e, i, r, a, n,  , X, u, ', ]</p>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">Abstract</h2>
    <p>As large language models (LLMs) continue to advance in capability and
influence, ensuring their security and preventing harmful outputs has become
crucial. A promising approach to address these concerns involves training
models to automatically generate adversarial prompts for red teaming. However,
the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness
of current adversarial methods, which struggle to specifically target and
explore the weaknesses of these models. To tackle these challenges, we
introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving
}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$
optimization framework, which enhances security by leveraging data generated by
the model itself. SEAS operates through three iterative stages: Initialization,
Attack, and Adversarial Optimization, refining both the Red Team and Target
models to improve robustness and safety. This framework reduces reliance on
manual testing and significantly enhances the security capabilities of LLMs.
Our contributions include a novel adversarial framework, a comprehensive safety
dataset, and after three iterations, the Target model achieves a security level
comparable to GPT-4, while the Red Team model shows a marked increase in attack
success rate (ASR) against advanced models.</p>
  </div>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">AI-Generated Summary</h2>
    <p>This paper mainly focuses on the security and safety of large language models, working on an adversarial framework and optimization for these models. It discusses generating adversarial prompts, but does not indicate these are hard prefix prompts or that there's a focus on prompt engineering.</p>
  </div>

  
    href="http://arxiv.org/pdf/2408.02632v1"
    className="bg-blue-500 text-white py-3 px-6 rounded-lg hover:bg-blue-600 transition duration-300 inline-block"
    target="_blank"
    rel="noopener noreferrer"
  >
    Read the full paper
  </a>

  <p className="text-sm text-gray-400 mt-8">
    The summary is AI-generated and may not perfectly reflect the paper's content.
  </p>
</div>
