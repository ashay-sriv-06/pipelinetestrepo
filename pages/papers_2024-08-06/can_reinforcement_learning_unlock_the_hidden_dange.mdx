---
title: "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?"
date: "2024-08-05"
authors: "[, ', M, o, h, a, m, m, a, d,  , B, a, h, r, a, m, i,  , K, a, r, k, e, v, a, n, d, i, ', ,,  , ', N, i, s, h, a, n, t,  , V, i, s, h, w, a, m, i, t, r, a, ', ,,  , ', P, e, y, m, a, n,  , N, a, j, a, f, i, r, a, d, ', ]"
---

import Image from 'next/image';

<div className="bg-gray-800 text-white min-h-screen p-8">
  <h1 className="text-4xl font-bold mb-6">Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?</h1>

  <div className="mb-8">
    <Image
      src="/images/papers/Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models_.png"
      alt="Paper image"
      width={800}
      height={600}
      layout="responsive"
      className="rounded-lg"
    />
    <p className="text-sm text-gray-400 mt-2">nan</p>
  </div>

  <p className="text-gray-300 mb-2"><strong>Date:</strong> 2024-08-05</p>
  <p className="text-gray-300 italic mb-6"><strong>Authors:</strong> [, ', M, o, h, a, m, m, a, d,  , B, a, h, r, a, m, i,  , K, a, r, k, e, v, a, n, d, i, ', ,,  , ', N, i, s, h, a, n, t,  , V, i, s, h, w, a, m, i, t, r, a, ', ,,  , ', P, e, y, m, a, n,  , N, a, j, a, f, i, r, a, d, ', ]</p>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">Abstract</h2>
    <p>Large Language Models (LLMs) have demonstrated impressive capabilities in
natural language tasks, but their safety and morality remain contentious due to
their training on internet text corpora. To address these concerns, alignment
techniques have been developed to improve the public usability and safety of
LLMs. Yet, the potential for generating harmful content through these models
seems to persist. This paper explores the concept of jailbreaking
LLMs-reversing their alignment through adversarial triggers. Previous methods,
such as soft embedding prompts, manually crafted prompts, and gradient-based
automatic prompts, have had limited success on black-box models due to their
requirements for model access and for producing a low variety of manually
crafted prompts, making them susceptible to being blocked. This paper
introduces a novel approach using reinforcement learning to optimize
adversarial triggers, requiring only inference API access to the target model
and a small surrogate model. Our method, which leverages a BERTScore-based
reward function, enhances the transferability and effectiveness of adversarial
triggers on new black-box models. We demonstrate that this approach improves
the performance of adversarial triggers on a previously untested language
model.</p>
  </div>

  <div className="bg-gray-700 p-6 rounded-lg mb-8">
    <h2 className="text-2xl font-semibold mb-4">AI-Generated Summary</h2>
    <p>The paper discusses methods of prompt generation such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts for language models, making it relevant to the topic of prompt engineering.</p>
  </div>

  
    href="http://arxiv.org/pdf/2408.02651v1"
    className="bg-blue-500 text-white py-3 px-6 rounded-lg hover:bg-blue-600 transition duration-300 inline-block"
    target="_blank"
    rel="noopener noreferrer"
  >
    Read the full paper
  </a>

  <p className="text-sm text-gray-400 mt-8">
    The summary is AI-generated and may not perfectly reflect the paper's content.
  </p>
</div>
