
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ARM: Efficient Guided Decoding with Autoregressive Reward Models</title>
        <style>
            body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 800px; margin: 0 auto; }
            h1 { color: #333; }
            .authors { font-style: italic; color: #666; margin-bottom: 20px; }
            .summary, .reasoning { background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
            .paper-link { display: inline-block; background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-top: 20px; }
            .paper-link:hover { background-color: #45a049; }
            .ai-notice { font-size: 0.8em; text-align: right; margin-top: 20px; color: #999; }
        </style>
    </head>
    <body>
        <h1>ARM: Efficient Guided Decoding with Autoregressive Reward Models</h1>
        <p><strong>Date:</strong> 2024-07-05</p>
        <p class="authors"><strong>Authors:</strong> ['Sergey Troshin', 'Vlad Niculae', 'Antske Fokkens']</p>
        <div class="summary">
            <h2>Abstract</h2>
            <p>Language models trained on large amounts of data require careful tuning to be
safely deployed in real world. We revisit the guided decoding paradigm, where
the goal is to augment the logits of the base language model using the scores
from a task-specific reward model. We propose a simple but efficient
parameterization of the autoregressive reward model enabling fast and effective
guided decoding. On detoxification and sentiment control tasks, we show that
our efficient parameterization performs on par with RAD, a strong but less
efficient guided decoding approach.</p>
        </div>
        <div class="reasoning">
            <h2>AI-Generated Summary</h2>
            <p>The paper discusses augmenting language models with task-specific reward models for guided decoding, a technique closely related to prompt engineering.</p>
        </div>
        <a href="http://arxiv.org/pdf/2407.04615v1" class="paper-link" target="_blank">Read the full paper</a>
        <p class="ai-notice">The summary is AI-generated and may not perfectly reflect the paper's content.</p>
    </body>
    </html>
    