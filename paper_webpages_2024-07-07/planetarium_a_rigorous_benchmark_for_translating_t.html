
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages</title>
        <style>
            body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 800px; margin: 0 auto; }
            h1 { color: #333; }
            .authors { font-style: italic; color: #666; margin-bottom: 20px; }
            .summary, .reasoning { background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
            .paper-link { display: inline-block; background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-top: 20px; }
            .paper-link:hover { background-color: #45a049; }
            .ai-notice { font-size: 0.8em; text-align: right; margin-top: 20px; color: #999; }
        </style>
    </head>
    <body>
        <h1>Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages</h1>
        <p><strong>Date:</strong> 2024-07-03</p>
        <p class="authors"><strong>Authors:</strong> ['Max Zuo', 'Francisco Piedrahita Velez', 'Xiaochen Li', 'Michael L. Littman', 'Stephen H. Bach']</p>
        <div class="summary">
            <h2>Abstract</h2>
            <p>Many recent works have explored using language models for planning problems.
One line of research focuses on translating natural language descriptions of
planning tasks into structured planning languages, such as the planning domain
definition language (PDDL). While this approach is promising, accurately
measuring the quality of generated PDDL code continues to pose significant
challenges. First, generated PDDL code is typically evaluated using planning
validators that check whether the problem can be solved with a planner. This
method is insufficient because a language model might generate valid PDDL code
that does not align with the natural language description of the task. Second,
existing evaluation sets often have natural language descriptions of the
planning task that closely resemble the ground truth PDDL, reducing the
challenge of the task. To bridge this gap, we introduce \benchmarkName, a
benchmark designed to evaluate language models' ability to generate PDDL code
from natural language descriptions of planning tasks. We begin by creating a
PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL
code generated by language models by flexibly comparing it against a ground
truth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across
13 different tasks, with varying levels of difficulty. Finally, we evaluate
several API-access and open-weight language models that reveal this task's
complexity. For example, $87.6\%$ of the PDDL problem descriptions generated by
GPT-4o are syntactically parseable, $82.2\%$ are valid, solve-able problems,
but only $35.1\%$ are semantically correct, highlighting the need for a more
rigorous benchmark for this problem.</p>
        </div>
        <div class="reasoning">
            <h2>AI-Generated Summary</h2>
            <p>The paper focuses on evaluating the ability of language models to generate PDDL code from natural language descriptions which aligns with the broader topic of prompt engineering.</p>
        </div>
        <a href="http://arxiv.org/pdf/2407.03321v1" class="paper-link" target="_blank">Read the full paper</a>
        <p class="ai-notice">The summary is AI-generated and may not perfectly reflect the paper's content.</p>
    </body>
    </html>
    