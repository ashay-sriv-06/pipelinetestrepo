
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing</title>
        <style>
            body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 800px; margin: 0 auto; }
            h1 { color: #333; }
            .authors { font-style: italic; color: #666; margin-bottom: 20px; }
            .summary, .reasoning { background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
            .paper-link { display: inline-block; background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-top: 20px; }
            .paper-link:hover { background-color: #45a049; }
            .ai-notice { font-size: 0.8em; text-align: right; margin-top: 20px; color: #999; }
        </style>
    </head>
    <body>
        <h1>Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing</h1>
        <p><strong>Date:</strong> 2024-07-03</p>
        <p class="authors"><strong>Authors:</strong> ['Egon Per≈°ak', 'Miguel F. Anjos', 'Sebastian Lautz', 'Aleksandar Kolev']</p>
        <div class="summary">
            <h2>Abstract</h2>
            <p>We propose a transformer architecture for time series forecasting with a
focus on time series tokenisation and apply it to a real-world prediction
problem from the pricing domain. Our architecture aims to learn effective
representations at many scales across all available data simultaneously. The
model contains a number of novel modules: a differentiated form of time series
patching which employs multiple resolutions, a multiple-resolution module for
time-varying known variables, a mixer-based module for capturing cross-series
information, and a novel output head with favourable scaling to account for the
increased number of tokens. We present an application of this model to a real
world prediction problem faced by the markdown team at a very large retailer.
On the experiments conducted our model outperforms in-house models and the
selected existing deep learning architectures.</p>
        </div>
        <div class="reasoning">
            <h2>AI-Generated Summary</h2>
            <p>The paper focuses on a transformer architecture for time series forecasting with a focus on tokenization, which is a key aspect of prompt engineering.</p>
        </div>
        <a href="http://arxiv.org/pdf/2407.03185v1" class="paper-link" target="_blank">Read the full paper</a>
        <p class="ai-notice">The summary is AI-generated and may not perfectly reflect the paper's content.</p>
    </body>
    </html>
    