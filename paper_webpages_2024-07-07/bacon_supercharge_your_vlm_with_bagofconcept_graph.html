
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations</title>
        <style>
            body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; max-width: 800px; margin: 0 auto; }
            h1 { color: #333; }
            .authors { font-style: italic; color: #666; margin-bottom: 20px; }
            .summary, .reasoning { background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
            .paper-link { display: inline-block; background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin-top: 20px; }
            .paper-link:hover { background-color: #45a049; }
            .ai-notice { font-size: 0.8em; text-align: right; margin-top: 20px; color: #999; }
        </style>
    </head>
    <body>
        <h1>BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations</h1>
        <p><strong>Date:</strong> 2024-07-03</p>
        <p class="authors"><strong>Authors:</strong> ['Zhantao Yang', 'Ruili Feng', 'Keyu Yan', 'Huangji Wang', 'Zhicai Wang', 'Shangwen Zhu', 'Han Zhang', 'Jie Xiao', 'Pingyu Wu', 'Kai Zhu', 'Jixuan Chen', 'Chen-Wei Xie', 'Chaojie Mao', 'Yue Yang', 'Hongyang Zhang', 'Yu Liu', 'Fan Cheng']</p>
        <div class="summary">
            <h2>Abstract</h2>
            <p>This paper presents Bag-of-Concept Graph (BACON) to gift models with limited
linguistic abilities to taste the privilege of Vision Language Models (VLMs)
and boost downstream tasks such as detection, visual question answering (VQA),
and image generation. Since the visual scenes in physical worlds are structured
with complex relations between objects, BACON breaks down annotations into
basic minimum elements and presents them in a graph structure. Element-wise
style enables easy understanding, and structural composition liberates
difficult locating. Careful prompt design births the BACON captions with the
help of public-available VLMs and segmentation methods. In this way, we gather
a dataset with 100K annotated images, which endow VLMs with remarkable
capabilities, such as accurately generating BACON, transforming prompts into
BACON format, envisioning scenarios in the style of BACONr, and dynamically
modifying elements within BACON through interactive dialogue and more. Wide
representative experiments, including detection, VQA, and image generation
tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel
in their current cutting-edge solutions.</p>
        </div>
        <div class="reasoning">
            <h2>AI-Generated Summary</h2>
            <p>The paper discusses the careful prompt design and transformation into a specific format (BACON) for enabling Vision Language Models, making it highly relevant to the topic of prompt engineering.</p>
        </div>
        <a href="http://arxiv.org/pdf/2407.03314v1" class="paper-link" target="_blank">Read the full paper</a>
        <p class="ai-notice">The summary is AI-generated and may not perfectly reflect the paper's content.</p>
    </body>
    </html>
    